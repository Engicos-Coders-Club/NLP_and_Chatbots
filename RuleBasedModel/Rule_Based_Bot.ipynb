{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqtHb_kdlqf2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BX3DMYeTcR-"
      },
      "source": [
        "# Data Prepocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDNQ_kmJYYlo"
      },
      "outputs": [],
      "source": [
        "# The dataset used for this code has a shape of 150000 rows by 3 columns, where the target variable is included in the third column.\n",
        "\n",
        "# code to convert the dialog.txt into intent.json format\n",
        "import json\n",
        "# This file was used to convert the simple dialogs.txt data into a intents.json type corpus.\n",
        "# read the dataset from a file\n",
        "with open('dialogs.txt', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "# create a list of intents\n",
        "intents = []\n",
        "for line in lines:\n",
        "  # split the line into input and output\n",
        "  input_text, output_text = line.strip().split('\\t')\n",
        "  # create an intent with a unique tag\n",
        "  intent = {\n",
        "    \"tag\": f\"intent_{len(intents)}\",\n",
        "    \"patterns\": [input_text],\n",
        "    \"responses\": [output_text]\n",
        "  }\n",
        "  intents.append(intent)\n",
        "\n",
        "# create the intents data structure\n",
        "intents = {\n",
        "  \"intents\": intents\n",
        "}\n",
        "\n",
        "# serialize the intents data structure to JSON\n",
        "intents_json = json.dumps(intents, indent=2)\n",
        "\n",
        "# write the intents data structure to the intents.json file\n",
        "with open('intents.json', 'w') as f:\n",
        "  f.write(intents_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKEDgklTxJjG"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjLQI2HUoemM"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize empty lists for training sentences, training labels, labels, and responses.\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "# 2. Open the `intents.json` file and load the data into a Python dictionary using the `json.load()` method.\n",
        "with open('intents.json') as file:\n",
        "    data = json.load(file)\n",
        "    # print(data)\n",
        "\n",
        "# 3. Loop through each intent in the data and extract the patterns and tag.\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        training_sentences.append(pattern)\n",
        "        training_labels.append(intent['tag'])\n",
        "        # 4. Append the responses to the `responses` list.\n",
        "    responses.append(intent['responses'])\n",
        "# 5. If the intent tag is not already in the `labels` list, append it.\n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])\n",
        "\n",
        "# print(training_labels)\n",
        "\n",
        "# 6. Get the number of classes (i.e., intents) by getting the length of the `labels` list.\n",
        "num_classes = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s4lt_xSsVx9"
      },
      "outputs": [],
      "source": [
        "# 7. Initialize a `LabelEncoder()` object and fit it to the training labels.\n",
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "\n",
        "# 8. Transform the training labels using the fitted label encoder.\n",
        "training_labels = lbl_encoder.transform(training_labels)\n",
        "\n",
        "# 9. Set the vocabulary size, embedding dimension, maximum sequence length, and out-of-vocabulary token.\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_len = 20\n",
        "oov_token = \"<OOV>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89M6l7o2suHY"
      },
      "outputs": [],
      "source": [
        "# 10. Initialize a `Tokenizer()` object with the specified vocabulary size and out-of-vocabulary token, and fit it to the training sentences.\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "# 11. Get the word index and convert the training sentences to sequences of integer indices using the fitted tokenizer.\n",
        "word_index = tokenizer.word_index \n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "\n",
        "# 12. Pad the sequences to ensure they have the same length using the specified maximum sequence length and truncation method.\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGJXshTHsype",
        "outputId": "0462520e-296e-45a2-aa01-214052b4eebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 20, 16)            16000     \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 16)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                272       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3725)              63325     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 79,869\n",
            "Trainable params: 79,869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 3s 10ms/step - loss: 8.2439 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 8.2277 - accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 1s 7ms/step - loss: 8.2276 - accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 8.2258 - accuracy: 2.6846e-04\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 8.1677 - accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 8.0271 - accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.9093 - accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.7353 - accuracy: 2.6846e-04\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.6266 - accuracy: 2.6846e-04\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.5502 - accuracy: 5.3691e-04\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.4708 - accuracy: 5.3691e-04\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.3859 - accuracy: 8.0537e-04\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.2962 - accuracy: 5.3691e-04\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.2054 - accuracy: 5.3691e-04\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.1110 - accuracy: 2.6846e-04\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 7.0155 - accuracy: 2.6846e-04\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 1s 8ms/step - loss: 6.9228 - accuracy: 2.6846e-04\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 6.8311 - accuracy: 8.0537e-04\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 6.7445 - accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 6.6683 - accuracy: 0.0013\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 6.5992 - accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 6.5379 - accuracy: 2.6846e-04\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 6.4857 - accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 1s 7ms/step - loss: 6.4373 - accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.3961 - accuracy: 2.6846e-04\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 6.3566 - accuracy: 0.0011\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.3216 - accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.2882 - accuracy: 2.6846e-04\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.2592 - accuracy: 0.0011\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.2313 - accuracy: 0.0016\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.2055 - accuracy: 5.3691e-04\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.1798 - accuracy: 8.0537e-04\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.1568 - accuracy: 0.0011\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.1336 - accuracy: 0.0016\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.1145 - accuracy: 8.0537e-04\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.0907 - accuracy: 0.0011\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 6.0707 - accuracy: 0.0011\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 6.0518 - accuracy: 0.0019\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 6.0320 - accuracy: 0.0013\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 6.0154 - accuracy: 0.0035\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 5.9946 - accuracy: 0.0019\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.9762 - accuracy: 0.0016\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.9610 - accuracy: 0.0024\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.9426 - accuracy: 0.0011\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.9256 - accuracy: 0.0024\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.9086 - accuracy: 0.0011\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.8926 - accuracy: 0.0019\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.8752 - accuracy: 0.0021\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.8610 - accuracy: 0.0019\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.8397 - accuracy: 0.0024\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.8244 - accuracy: 0.0030\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.8102 - accuracy: 0.0032\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.7922 - accuracy: 0.0040\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.7745 - accuracy: 0.0027\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 1s 8ms/step - loss: 5.7579 - accuracy: 0.0030\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 5.7402 - accuracy: 0.0030\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 5.7266 - accuracy: 0.0035\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 5.7065 - accuracy: 0.0027\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 1s 8ms/step - loss: 5.6886 - accuracy: 0.0038\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.6718 - accuracy: 0.0027\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.6546 - accuracy: 0.0043\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.6369 - accuracy: 0.0032\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.6153 - accuracy: 0.0051\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.5990 - accuracy: 0.0046\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.5804 - accuracy: 0.0048\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.5631 - accuracy: 0.0054\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.5421 - accuracy: 0.0064\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.5253 - accuracy: 0.0056\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.5035 - accuracy: 0.0078\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.4827 - accuracy: 0.0054\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.4646 - accuracy: 0.0070\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.4482 - accuracy: 0.0081\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 1s 8ms/step - loss: 5.4255 - accuracy: 0.0067\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 5.4072 - accuracy: 0.0062\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 5.3849 - accuracy: 0.0070\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 5.3650 - accuracy: 0.0072\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 1s 8ms/step - loss: 5.3447 - accuracy: 0.0086\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.3272 - accuracy: 0.0051\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.3050 - accuracy: 0.0064\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.2846 - accuracy: 0.0075\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.2645 - accuracy: 0.0075\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.2468 - accuracy: 0.0070\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.2263 - accuracy: 0.0089\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.2111 - accuracy: 0.0075\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.1910 - accuracy: 0.0089\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.1719 - accuracy: 0.0081\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.1520 - accuracy: 0.0097\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.1369 - accuracy: 0.0107\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.1206 - accuracy: 0.0083\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.1084 - accuracy: 0.0091\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 5.0838 - accuracy: 0.0105\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 5.0658 - accuracy: 0.0081\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 5.0497 - accuracy: 0.0097\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 5.0307 - accuracy: 0.0118\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 5.0165 - accuracy: 0.0113\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 4.9979 - accuracy: 0.0118\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 1s 7ms/step - loss: 4.9801 - accuracy: 0.0129\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 4.9613 - accuracy: 0.0142\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 4.9500 - accuracy: 0.0107\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 1s 6ms/step - loss: 4.9342 - accuracy: 0.0118\n"
          ]
        }
      ],
      "source": [
        "# 13. Define a sequential model with an embedding layer, global average pooling layer, two dense layers with ReLU activation, and a dense output layer with softmax activation.\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 14. Compile the model with sparse categorical cross-entropy loss, Adam optimizer, and accuracy metric.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 15. Print a summary of the model architecture.\n",
        "model.summary()\n",
        "\n",
        "# 16. Train the model on the padded sequences and corresponding training labels for the specified number of epochs.\n",
        "epochs = 100\n",
        "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwMCNWkEtOFt",
        "outputId": "6d09b9c3-7615-47fa-901d-4a48164d62d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# to save the trained model\n",
        "model.save(\"chat_model\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# to save the fitted tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# to save the fitted label encoder\n",
        "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
        "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGXBf6HAjYoC",
        "outputId": "e0cfe9cf-f054-4ca2-f22c-2f4b31932470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "pip install colorama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ihqHDs3W3rv"
      },
      "source": [
        "## File to use the built model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZwfTpm3jKWo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import colorama\n",
        "\n",
        "colorama.init()\n",
        "from colorama import Fore, Style, Back\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "with open(\"intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "\n",
        "def chat():\n",
        "    # load trained model\n",
        "    model = keras.models.load_model('chat_model')\n",
        "    # load tokenizer object\n",
        "    with open('tokenizer.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "    # load label encoder object\n",
        "    with open('label_encoder.pickle', 'rb') as enc:\n",
        "        lbl_encoder = pickle.load(enc)\n",
        "    # parameters\n",
        "    max_len = 20\n",
        "    while True:\n",
        "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
        "        inp = input()\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),truncating='post', maxlen=max_len))\n",
        "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
        "        for i in data['intents']:\n",
        "            if i['tag'] == tag:\n",
        "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL, np.random.choice(i['responses']))\n",
        "                break\n",
        "        # print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(responses))\n",
        "\n",
        "\n",
        "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
        "chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06sXjepkpz1i"
      },
      "source": [
        "# Using OpenAI GPT 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fPiqyHSm89C",
        "outputId": "10adbc2e-f344-4fd0-f54e-922dba5026e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User : so what are tonight\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"so what are tonight's goals, in the hope that one day we can get over this hurdle? You know what I want for tonight? It's\"}, {'generated_text': 'so what are tonight\\'s questions asked?\" he asked.\\n\\n\"Can we go inside for an interview?\" asked the former secretary, as his body'}]\n",
            "User : you know the code\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"you know the code can be tricky. We have the full source code for each piece in the project. So what's more important:\\n\\nIf\"}, {'generated_text': 'you know the code for this. (And also the code for this.) The more we read, the greater the likelihood of this happening. But we'}]\n",
            "User : kill\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "while 1:\n",
        "  inp = input(\"User : \")\n",
        "  if inp==\"kill\":\n",
        "    break\n",
        "  out = generator(inp, max_length=30, num_return_sequences=2)\n",
        "  print(out)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}